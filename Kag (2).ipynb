{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting options\n",
    "mpl.style.use('ggplot')\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv(r'C:\\Users\\Alvi Mahmud\\Desktop\\BAN Sp20\\Kag\\train.csv')\n",
    "df_test=pd.read_csv(r'C:\\Users\\Alvi Mahmud\\Desktop\\BAN Sp20\\Kag\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600000 entries, 0 to 599999\n",
      "Data columns (total 25 columns):\n",
      "id        600000 non-null int64\n",
      "bin_0     582106 non-null float64\n",
      "bin_1     581997 non-null float64\n",
      "bin_2     582070 non-null float64\n",
      "bin_3     581986 non-null object\n",
      "bin_4     581953 non-null object\n",
      "nom_0     581748 non-null object\n",
      "nom_1     581844 non-null object\n",
      "nom_2     581965 non-null object\n",
      "nom_3     581879 non-null object\n",
      "nom_4     581965 non-null object\n",
      "nom_5     582222 non-null object\n",
      "nom_6     581869 non-null object\n",
      "nom_7     581997 non-null object\n",
      "nom_8     582245 non-null object\n",
      "nom_9     581927 non-null object\n",
      "ord_0     581712 non-null float64\n",
      "ord_1     581959 non-null object\n",
      "ord_2     581925 non-null object\n",
      "ord_3     582084 non-null object\n",
      "ord_4     582070 non-null object\n",
      "ord_5     582287 non-null object\n",
      "day       582048 non-null float64\n",
      "month     582012 non-null float64\n",
      "target    600000 non-null int64\n",
      "dtypes: float64(6), int64(2), object(17)\n",
      "memory usage: 114.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400000 entries, 0 to 399999\n",
      "Data columns (total 24 columns):\n",
      "id       400000 non-null int64\n",
      "bin_0    388099 non-null float64\n",
      "bin_1    387962 non-null float64\n",
      "bin_2    388028 non-null float64\n",
      "bin_3    388049 non-null object\n",
      "bin_4    388049 non-null object\n",
      "nom_0    387938 non-null object\n",
      "nom_1    388053 non-null object\n",
      "nom_2    387821 non-null object\n",
      "nom_3    387824 non-null object\n",
      "nom_4    388007 non-null object\n",
      "nom_5    388088 non-null object\n",
      "nom_6    387988 non-null object\n",
      "nom_7    387997 non-null object\n",
      "nom_8    388044 non-null object\n",
      "nom_9    387940 non-null object\n",
      "ord_0    388107 non-null float64\n",
      "ord_1    387833 non-null object\n",
      "ord_2    387895 non-null object\n",
      "ord_3    387947 non-null object\n",
      "ord_4    388067 non-null object\n",
      "ord_5    387953 non-null object\n",
      "day      387975 non-null float64\n",
      "month    388016 non-null float64\n",
      "dtypes: float64(6), int64(1), object(17)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1',\n",
      "       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9',\n",
      "       'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month',\n",
      "       'target'],\n",
      "      dtype='object')\n",
      "   id  bin_0  bin_1  bin_2 bin_3 bin_4 nom_0      nom_1    nom_2   nom_3  ...  \\\n",
      "0   0    0.0    0.0    0.0     F     N   Red  Trapezoid  Hamster  Russia  ...   \n",
      "1   1    1.0    1.0    0.0     F     Y   Red       Star  Axolotl     NaN  ...   \n",
      "2   2    0.0    1.0    0.0     F     N   Red        NaN  Hamster  Canada  ...   \n",
      "\n",
      "       nom_9 ord_0        ord_1     ord_2 ord_3 ord_4  ord_5  day month target  \n",
      "0  02e7c8990   3.0  Contributor       Hot     c     U     Pw  6.0   3.0      0  \n",
      "1  f37df64af   3.0  Grandmaster      Warm     e     X     pE  7.0   7.0      0  \n",
      "2        NaN   3.0          NaN  Freezing     n     P     eN  5.0   9.0      0  \n",
      "\n",
      "[3 rows x 25 columns]\n",
      "       id  bin_0  bin_1  bin_2 bin_3 bin_4 nom_0    nom_1    nom_2    nom_3  \\\n",
      "0  600000    0.0    0.0    0.0     F     Y  Blue  Polygon  Axolotl  Finland   \n",
      "1  600001    0.0    0.0    0.0     F     Y   Red   Circle     Lion   Russia   \n",
      "2  600002    0.0    0.0    0.0     F     Y  Blue   Circle  Axolotl   Russia   \n",
      "\n",
      "   ...      nom_8      nom_9 ord_0   ord_1        ord_2 ord_3  ord_4 ord_5  \\\n",
      "0  ...  ca9ad1d4b  fced9e114   3.0  Novice  Boiling Hot     f      U    oU   \n",
      "1  ...  060a21580  7ca8775da   1.0  Novice         Cold     n      N   NaN   \n",
      "2  ...  165e81a00  5940334c9   1.0  Expert         Warm     i      N    DN   \n",
      "\n",
      "   day month  \n",
      "0  3.0   9.0  \n",
      "1  2.0   8.0  \n",
      "2  2.0   6.0  \n",
      "\n",
      "[3 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)\n",
    "print(df_train.head(3))\n",
    "print(df_test.head(3))\n",
    "\n",
    "#Mask the Null values to retain them during encoding\n",
    "mask_train= df_train.isin(['nan'])\n",
    "mask_test= df_test.isin(['nan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert object to string\n",
    "df_train[['bin_3','bin_4','nom_0', 'nom_1',\n",
    "       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']]= df_train[['bin_3','bin_4','nom_0', 'nom_1',\n",
    "       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']].astype('str')\n",
    "\n",
    "df_test[['bin_3','bin_4','nom_0', 'nom_1',\n",
    "       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']]= df_test[['bin_3','bin_4','nom_0', 'nom_1',\n",
    "       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode all the nominal features and the two string binary feature\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "df_train_enc= MultiColumnLabelEncoder(columns = [ 'bin_3','bin_4','nom_0', 'nom_1',\n",
    "       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']).fit_transform(df_train)\n",
    "df_train_enc=pd.DataFrame(df_train_enc.where(~mask_train, other=np.nan))\n",
    "\n",
    "df_test_enc= MultiColumnLabelEncoder(columns = [ 'bin_3','bin_4','nom_0', 'nom_1',\n",
    "       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']).fit_transform(df_test)\n",
    "df_test_enc=pd.DataFrame(df_test_enc.where(~mask_test, other=np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    312344\n",
       "1    269609\n",
       "2     18047\n",
       "Name: bin_4, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_enc.bin_4.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map low cardinality ordinal features\n",
    "map_ord1 = {'Novice':1, \n",
    "            'Contributor':2, \n",
    "            'Expert':3, \n",
    "            'Master':4, \n",
    "            'Grandmaster':5}\n",
    "df_train_enc.ord_1 = df_train_enc.ord_1.replace(map_ord1)\n",
    "df_test_enc.ord_1 = df_test_enc.ord_1.replace(map_ord1)\n",
    "\n",
    "map_ord2 = {'Freezing':1, \n",
    "            'Cold':10, \n",
    "            'Warm':25, \n",
    "            'Hot':50, \n",
    "            'Boiling Hot':100, \n",
    "            'Lava Hot':800}\n",
    "df_train_enc.ord_2 = df_train_enc.ord_2.replace(map_ord2)\n",
    "df_test_enc.ord_2 = df_test_enc.ord_2.replace(map_ord2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode high cardinality features\n",
    "map_ord3 = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':6, 'g':7, 'h':8, 'i':9, 'j':10, 'k':11, 'l':12,'m':13,'n':14, 'o':15}\n",
    "df_train_enc.ord_3 = df_train_enc.ord_3.replace(map_ord3)\n",
    "df_test_enc.ord_3 = df_test_enc.ord_3.replace(map_ord3)\n",
    "\n",
    "map_ord4 = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'H':8, 'I':9, 'J':10, 'K':11, 'L':12,'M':13,'N':14, 'O':15,\n",
    "           'P':16, 'Q':17, 'R':18, 'S':19, 'T':20, 'U':21, 'V':22, 'W':23, 'X':24, 'Y':25, 'Z':26}\n",
    "df_train_enc.ord_4 = df_train_enc.ord_4.replace(map_ord4)\n",
    "df_test_enc.ord_4 = df_test_enc.ord_4.replace(map_ord4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fl    10562\n",
       "DN     9527\n",
       "Sz     8654\n",
       "RV     5648\n",
       "oJ     5596\n",
       "      ...  \n",
       "vw      189\n",
       "gV      124\n",
       "vQ      120\n",
       "eA       91\n",
       "Zv       87\n",
       "Name: ord_5_enc, Length: 190, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "df_train_enc['ord_5_enc']=df_train_enc['ord_5']\n",
    "df_test_enc['ord_5_enc']=df_test_enc['ord_5']\n",
    "ce_ord = ce.OrdinalEncoder(cols = ['ord_5'])\n",
    "df_train_encall=ce_ord.fit_transform(df_train_enc, df_train_enc['ord_5_enc'])\n",
    "df_test_encall=ce_ord.fit_transform(df_test_enc, df_test_enc['ord_5_enc'])\n",
    "df_train_encall.ord_5_enc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_encall.ord_5.value_counts()\n",
    "df_train_encoded= df_train_encall.drop(['target','ord_5_enc'], axis=1)\n",
    "df_test_encoded= df_test_encall.drop('ord_5_enc', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  bin_0  bin_1  bin_2  bin_3  bin_4  nom_0  nom_1  nom_2  nom_3  ...  \\\n",
      "0   0    0.0    0.0    0.0      0      0      2      4      3      5  ...   \n",
      "1   1    1.0    1.0    0.0      0      1      2      3      0      6  ...   \n",
      "2   2    0.0    1.0    0.0      0      0      2      6      3      0  ...   \n",
      "\n",
      "   nom_8  nom_9  ord_0  ord_1  ord_2  ord_3  ord_4  ord_5  day  month  \n",
      "0      1     27    3.0    2.0   50.0    3.0   21.0      1  6.0    3.0  \n",
      "1     69   2112    3.0    5.0   25.0    5.0   24.0      2  7.0    7.0  \n",
      "2    102   2218    3.0    NaN    1.0   14.0   16.0      3  5.0    9.0  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600000 entries, 0 to 599999\n",
      "Data columns (total 24 columns):\n",
      "id       600000 non-null int64\n",
      "bin_0    582106 non-null float64\n",
      "bin_1    581997 non-null float64\n",
      "bin_2    582070 non-null float64\n",
      "bin_3    600000 non-null int32\n",
      "bin_4    600000 non-null int32\n",
      "nom_0    600000 non-null int32\n",
      "nom_1    600000 non-null int32\n",
      "nom_2    600000 non-null int32\n",
      "nom_3    600000 non-null int32\n",
      "nom_4    600000 non-null int32\n",
      "nom_5    600000 non-null int32\n",
      "nom_6    600000 non-null int32\n",
      "nom_7    600000 non-null int32\n",
      "nom_8    600000 non-null int32\n",
      "nom_9    600000 non-null int32\n",
      "ord_0    581712 non-null float64\n",
      "ord_1    581959 non-null float64\n",
      "ord_2    581925 non-null float64\n",
      "ord_3    582084 non-null float64\n",
      "ord_4    582070 non-null float64\n",
      "ord_5    600000 non-null int32\n",
      "day      582048 non-null float64\n",
      "month    582012 non-null float64\n",
      "dtypes: float64(10), int32(13), int64(1)\n",
      "memory usage: 80.1 MB\n"
     ]
    }
   ],
   "source": [
    "df_train_encoded.columns\n",
    "print(df_train_encoded.head(3))\n",
    "df_train_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent') #for median imputation replace 'mean' with 'median'\n",
    "imp_mean.fit(df_train_encoded)\n",
    "df_train_imputed=pd.DataFrame(imp_mean.transform(df_train_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mean2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp_mean2.fit(df_test_encoded)\n",
    "df_test_imputed=pd.DataFrame(imp_mean2.transform(df_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0    1    2    3    4    5    6    7    8    9   ...     14      15   16  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  4.0  3.0  5.0  ...    1.0    27.0  3.0   \n",
      "1  1.0  1.0  1.0  0.0  0.0  1.0  2.0  3.0  0.0  6.0  ...   69.0  2112.0  3.0   \n",
      "2  2.0  0.0  1.0  0.0  0.0  0.0  2.0  6.0  3.0  0.0  ...  102.0  2218.0  3.0   \n",
      "\n",
      "    17    18    19    20   21   22   23  \n",
      "0  2.0  50.0   3.0  21.0  1.0  6.0  3.0  \n",
      "1  5.0  25.0   5.0  24.0  2.0  7.0  7.0  \n",
      "2  1.0   1.0  14.0  16.0  3.0  5.0  9.0  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "         0    1    2    3    4    5    6    7    8    9   ...     14      15  \\\n",
      "0  600000.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  3.0  ...  174.0  2194.0   \n",
      "1  600001.0  0.0  0.0  0.0  0.0  1.0  2.0  0.0  4.0  5.0  ...    4.0  1105.0   \n",
      "2  600002.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  5.0  ...   16.0   810.0   \n",
      "\n",
      "    16   17     18    19    20    21   22   23  \n",
      "0  3.0  1.0  100.0   6.0  21.0  49.0  3.0  9.0  \n",
      "1  1.0  1.0   10.0  14.0  14.0   4.0  2.0  8.0  \n",
      "2  1.0  3.0   25.0   9.0  14.0  38.0  2.0  6.0  \n",
      "\n",
      "[3 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train_imputed.head(3))\n",
    "print(df_test_imputed.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    244092\n",
       "1.0    143957\n",
       "2.0     11951\n",
       "Name: bin_3, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_imputed.columns=df_train_encoded.columns\n",
    "df_test_imputed.columns=df_test_encoded.columns\n",
    "\n",
    "df_test_imputed.bin_3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       float64\n",
      "bin_0    float64\n",
      "bin_1    float64\n",
      "bin_2    float64\n",
      "bin_3    float64\n",
      "bin_4    float64\n",
      "nom_0    float64\n",
      "nom_1    float64\n",
      "nom_2    float64\n",
      "nom_3    float64\n",
      "nom_4    float64\n",
      "nom_5    float64\n",
      "nom_6    float64\n",
      "nom_7    float64\n",
      "nom_8    float64\n",
      "nom_9    float64\n",
      "ord_0    float64\n",
      "ord_1    float64\n",
      "ord_2    float64\n",
      "ord_3    float64\n",
      "ord_4    float64\n",
      "ord_5    float64\n",
      "day      float64\n",
      "month    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train_imputed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       float64\n",
      "bin_0    float64\n",
      "bin_1    float64\n",
      "bin_2    float64\n",
      "bin_3    float64\n",
      "bin_4    float64\n",
      "nom_0    float64\n",
      "nom_1    float64\n",
      "nom_2    float64\n",
      "nom_3    float64\n",
      "nom_4    float64\n",
      "nom_5    float64\n",
      "nom_6    float64\n",
      "nom_7    float64\n",
      "nom_8    float64\n",
      "nom_9    float64\n",
      "ord_0    float64\n",
      "ord_1    float64\n",
      "ord_2    float64\n",
      "ord_3    float64\n",
      "ord_4    float64\n",
      "ord_5    float64\n",
      "day      float64\n",
      "month    float64\n",
      "dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_test_imputed.dtypes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train_imputed,df_train_encall.target, test_size=0.25, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvi Mahmud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Alvi Mahmud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Alvi Mahmud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Alvi Mahmud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Alvi Mahmud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Alvi Mahmud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Alvi Mahmud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\Alvi Mahmud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "smote_nc = SMOTENC(categorical_features=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23], random_state=420)\n",
    "X_train_bl, y_train_bl = smote_nc.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r'C:\\Users\\Alvi Mahmud\\Desktop\\BAN Sp20\\Kag\\imp_bal_X.npy', X_train_bl)\n",
    "np.save(r'C:\\Users\\Alvi Mahmud\\Desktop\\BAN Sp20\\Kag\\imp_bal_Y.npy', y_train_bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bl= np.load(r'C:\\Users\\Alvi Mahmud\\Desktop\\BAN Sp20\\Kag\\imp_bal_X.npy', allow_pickle=True) \n",
    "y_train_bl=np.load(r'C:\\Users\\Alvi Mahmud\\Desktop\\BAN Sp20\\Kag\\imp_bal_Y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2=pd.DataFrame(X_train_bl)\n",
    "df_train2.columns= df_train_imputed.columns\n",
    "trn= pd.get_dummies(df_train2,columns = [ 'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1',\n",
    "       'nom_2', 'nom_3', 'nom_4', 'ord_0', 'ord_1', 'ord_2'],drop_first=True)\n",
    "val= pd.get_dummies(X_val,columns = [ 'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1',\n",
    "       'nom_2', 'nom_3', 'nom_4', 'ord_0', 'ord_1', 'ord_2'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731238, 54)\n",
      "(150000, 54)\n",
      "Index(['id', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4',\n",
      "       'ord_5', 'day', 'month', 'bin_0_1.0', 'bin_1_1.0', 'bin_2_1.0',\n",
      "       'bin_3_1.0', 'bin_3_2.0', 'bin_4_1.0', 'bin_4_2.0', 'nom_0_1.0',\n",
      "       'nom_0_2.0', 'nom_0_3.0', 'nom_1_1.0', 'nom_1_2.0', 'nom_1_3.0',\n",
      "       'nom_1_4.0', 'nom_1_5.0', 'nom_1_6.0', 'nom_2_1.0', 'nom_2_2.0',\n",
      "       'nom_2_3.0', 'nom_2_4.0', 'nom_2_5.0', 'nom_2_6.0', 'nom_3_1.0',\n",
      "       'nom_3_2.0', 'nom_3_3.0', 'nom_3_4.0', 'nom_3_5.0', 'nom_3_6.0',\n",
      "       'nom_4_1.0', 'nom_4_2.0', 'nom_4_3.0', 'nom_4_4.0', 'ord_0_2.0',\n",
      "       'ord_0_3.0', 'ord_1_2.0', 'ord_1_3.0', 'ord_1_4.0', 'ord_1_5.0',\n",
      "       'ord_2_10.0', 'ord_2_25.0', 'ord_2_50.0', 'ord_2_100.0', 'ord_2_800.0'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(trn.shape)\n",
    "print(val.shape)\n",
    "print(val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.get_dummies(df_test_imputed,columns = [ 'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1',\n",
    "       'nom_2', 'nom_3', 'nom_4', 'ord_0', 'ord_1', 'ord_2'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 54)\n",
      "Index(['id', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4',\n",
      "       'ord_5', 'day', 'month', 'bin_0_1.0', 'bin_1_1.0', 'bin_2_1.0',\n",
      "       'bin_3_1.0', 'bin_3_2.0', 'bin_4_1.0', 'bin_4_2.0', 'nom_0_1.0',\n",
      "       'nom_0_2.0', 'nom_0_3.0', 'nom_1_1.0', 'nom_1_2.0', 'nom_1_3.0',\n",
      "       'nom_1_4.0', 'nom_1_5.0', 'nom_1_6.0', 'nom_2_1.0', 'nom_2_2.0',\n",
      "       'nom_2_3.0', 'nom_2_4.0', 'nom_2_5.0', 'nom_2_6.0', 'nom_3_1.0',\n",
      "       'nom_3_2.0', 'nom_3_3.0', 'nom_3_4.0', 'nom_3_5.0', 'nom_3_6.0',\n",
      "       'nom_4_1.0', 'nom_4_2.0', 'nom_4_3.0', 'nom_4_4.0', 'ord_0_2.0',\n",
      "       'ord_0_3.0', 'ord_1_2.0', 'ord_1_3.0', 'ord_1_4.0', 'ord_1_5.0',\n",
      "       'ord_2_10.0', 'ord_2_25.0', 'ord_2_50.0', 'ord_2_100.0', 'ord_2_800.0'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731238, 53)\n",
      "(150000, 53)\n",
      "(400000, 53)\n"
     ]
    }
   ],
   "source": [
    "train= trn.drop('id',axis=1)\n",
    "val= val.drop('id',axis=1)\n",
    "test= test.drop('id',axis=1)\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "#print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "SC= StandardScaler()\n",
    "train_sc = SC.fit_transform(train)\n",
    "val_sc=SC.fit_transform(val)\n",
    "test_sc=SC.fit_transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gcv=pd.DataFrame(train_sc)\n",
    "train_gcv.columns=train.columns\n",
    "val_gcv=pd.DataFrame(val_sc)\n",
    "val_gcv.columns=val.columns\n",
    "test_gcv=pd.DataFrame(test_sc)\n",
    "test_gcv.columns=test.columns\n",
    "target=pd.Series(y_train_bl).rename('target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "param2 = {'max_depth':[10,20,30], 'subsample':[0.7], 'booster':['dart'], 'tree_method': ['hist'],\n",
    "         'rate_drop': [0.1],'gamma':[0.01,0.1,1],'learning_rate': [0.2],\n",
    "          'min_child_weight': [5,10,30],'n_estimators': [50], 'seed':[420]}\n",
    "fit_params={\"early_stopping_rounds\":[20], \"eval_metric\":[\"auc\"], \"eval_set\" : [[val_gcv, y_val]]}   \n",
    "#model2= xgb.train(dtrain=dtrain,params= param2,num_boost_round=250,early_stopping_rounds=20,\n",
    "                        #evals= [(dval, 'eval'), (dtrain, 'train')], verbose_eval=10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf = GridSearchCV(xgb_model, param2, verbose=2, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.9min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.1min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.9min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.4min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.4min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.8min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.9min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 8.0min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.9min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 5.6min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.2min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.3min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.4min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.4min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.3min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.6min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.6min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 8.3min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 9.4min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 9.4min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 9.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 9.6min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.8min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.5min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.7min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.8min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.8min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.9min\n",
      "[CV] booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.01, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.9min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.4min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.4min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.6min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.4min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 2.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.3min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.6min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.8min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.8min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.9min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 5.4min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.3min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.3min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.4min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.4min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.2min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 8.2min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 9.4min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 9.4min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 9.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=12.0min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=10.9min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=11.5min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=13.2min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=46.6min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=11.7min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.6min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.7min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.7min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.8min\n",
      "[CV] booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=0.1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 7.8min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.8min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.1min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.0min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.1min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.1min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.8min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.9min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.0min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.9min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.9min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.7min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.9min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.9min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.9min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=10, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 3.9min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 9.6min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=11.6min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=11.6min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=11.7min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=10.0min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 5.4min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.2min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.4min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.4min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 6.6min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.2min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.5min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.5min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.6min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=20, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 4.5min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 8.3min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 9.3min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=23.9min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=10.9min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=5, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=10.9min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total= 9.4min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=13.1min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=16.6min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=16.5min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=10, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=19.0min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=12.8min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=10.1min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=11.0min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=11.6min\n",
      "[CV] booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist \n",
      "[CV]  booster=dart, gamma=1, learning_rate=0.2, max_depth=30, min_child_weight=30, n_estimators=50, rate_drop=0.1, seed=420, subsample=0.7, tree_method=hist, total=12.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed: 912.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale...\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'booster': ['dart'], 'gamma': [0.01, 0.1, 1],\n",
       "                         'learning_rate': [0.2], 'max_depth': [10, 20, 30],\n",
       "                         'min_child_weight': [5, 10, 30], 'n_estimators': [50],\n",
       "                         'rate_drop': [0.1], 'seed': [420], 'subsample': [0.7],\n",
       "                         'tree_method': ['hist']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_gcv, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.333327\teval-auc:0.601355\ttrain-error:0.128578\ttrain-auc:0.924037\n",
      "Multiple eval metrics have been passed: 'train-auc' will be used for early stopping.\n",
      "\n",
      "Will train until train-auc hasn't improved in 20 rounds.\n",
      "[10]\teval-error:0.27676\teval-auc:0.645921\ttrain-error:0.026636\ttrain-auc:0.996286\n",
      "[20]\teval-error:0.272\teval-auc:0.653235\ttrain-error:0.015375\ttrain-auc:0.998715\n",
      "[30]\teval-error:0.26492\teval-auc:0.661881\ttrain-error:0.007995\ttrain-auc:0.99947\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(train_sc, label=y_train_bl)\n",
    "dval = xgb.DMatrix(val_sc, label=y_val.values)\n",
    "dtest = xgb.DMatrix(test_sc)\n",
    "\n",
    "\n",
    "param = {'max_depth':30, 'silent':1, 'objective':'binary:logistic', 'subsample':0.7,\"booster\": 'dart',  'tree_method': 'hist', \n",
    "         'sample_type': 'weighted', 'eval_metric':[\"error\",\"auc\"],'learning_rate': 0.25, 'n_estimator':200,\n",
    "         'rate_drop': 0.1, 'seed':420}\n",
    "\n",
    "model= xgb.train(dtrain=dtrain,params= param,num_boost_round=250,early_stopping_rounds=20,\n",
    "                        evals= [(dval, 'eval'), (dtrain, 'train')], verbose_eval=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(dtest, ntree_limit= 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= pd.concat([pd.Series(y_pred), df_test.id],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0      id\n",
      "0       0.207773  600000\n",
      "1       0.590388  600001\n",
      "2       0.585616  600002\n",
      "3       0.073573  600003\n",
      "4       0.245190  600004\n",
      "...          ...     ...\n",
      "399995  0.667515  999995\n",
      "399996  0.440480  999996\n",
      "399997  0.852044  999997\n",
      "399998  0.726058  999998\n",
      "399999  0.250907  999999\n",
      "\n",
      "[400000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xgb= result.values\n",
    "np.save(r'C:\\Users\\Alvi Mahmud\\Desktop\\BAN Sp20\\Kag\\xgb2.npy', result_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission =pd.DataFrame(np.load(r'C:\\Users\\Alvi Mahmud\\Desktop\\BAN Sp20\\Kag\\xgb.npy', allow_pickle=True), columns= ('target', 'id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          target      id\n",
      "0       0.001812  600000\n",
      "1       0.308687  600001\n",
      "2       0.806775  600002\n",
      "3       0.014121  600003\n",
      "4       0.002912  600004\n",
      "...          ...     ...\n",
      "399995  0.846877  999995\n",
      "399996  0.077470  999996\n",
      "399997  0.904900  999997\n",
      "399998  0.146185  999998\n",
      "399999  0.327749  999999\n",
      "\n",
      "[400000 rows x 2 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400000 entries, 0 to 399999\n",
      "Data columns (total 2 columns):\n",
      "target    400000 non-null float64\n",
      "id        400000 non-null int32\n",
      "dtypes: float64(1), int32(1)\n",
      "memory usage: 4.6 MB\n"
     ]
    }
   ],
   "source": [
    "submission.id= submission.id.astype(int)\n",
    "print(submission)\n",
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(r'C:\\Users\\Alvi Mahmud\\Desktop\\BAN Sp20\\Kag\\Submission_Xgb', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(r'C:\\Users\\Alvi Mahmud\\Desktop\\BAN Sp20\\Kag\\Submission_Xgb3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
